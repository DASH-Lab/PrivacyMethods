{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.447), (0.247, 0.243, 0.262))\n",
    "])\n",
    "\n",
    "root = '' ## 이미지 경로\n",
    "\n",
    "original_trainset = datasets.ImageFolder(root=root+\"/origin-cifar-10/train\", transform=trans)\n",
    "original_testset = datasets.ImageFolder(root=root+\"origin-cifar-10/test\", transform=trans)\n",
    "svd_trainset = datasets.ImageFolder(root=root+\"/svd-cifar-10/train\", transform=trans)\n",
    "svd_testset = datasets.ImageFolder(root=root+\"/svd-cifar-10/test\", transform=trans)\n",
    "svd4_trainset = datasets.ImageFolder(root=root+\"/svd4-cifar-10/train\", transform=trans)\n",
    "svd4_testset = datasets.ImageFolder(root=root+\"/svd4-cifar-10/test\", transform=trans)\n",
    "\n",
    "o_trainloader = DataLoader(original_trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "o_testloader = DataLoader(original_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "s_trainloader = DataLoader(svd_trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "s_testloader = DataLoader(svd_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "s4_trainloader = DataLoader(svd4_trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "s4_testloader = DataLoader(svd4_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model1 = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "model2 = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "model3 = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "\n",
    "model1.fc = nn.Linear(in_features=model1.fc.in_features, out_features=10, bias=True)\n",
    "model2.fc = nn.Linear(in_features=model2.fc.in_features, out_features=10, bias=True)\n",
    "model3.fc = nn.Linear(in_features=model3.fc.in_features, out_features=10, bias=True)\n",
    "\n",
    "nn.init.xavier_normal_(model1.fc.weight)\n",
    "nn.init.xavier_normal_(model2.fc.weight)\n",
    "nn.init.xavier_normal_(model3.fc.weight)\n",
    "\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    loss_sum = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred, _ = model(X.to(device))\n",
    "        loss = criterion(pred, y.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            loss_sum += loss\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return loss_sum / size\n",
    "\n",
    "def test_loop(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    hit, loss = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            loss += criterion(pred, y.to(device)).item()\n",
    "            hit += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "    \n",
    "    loss /= (size/batch_size)\n",
    "    hit /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*hit):>0.1f}%, Avg loss: {loss:>8f}\\n\")\n",
    "\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_loss1, test_acc1, test_acc2, test_acc3 = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i + 1}-------------------------------\")\n",
    "    model1.train()\n",
    "    train_loss1.append(train_loop(o_trainloader, model1, criterion, optimizer1))\n",
    "    model1.eval()\n",
    "    test_acc1.append(test_loop(o_testloader, model1, criterion))\n",
    "    test_acc2.append(test_loop(s_testloader, model1, criterion))\n",
    "    test_acc3.append(test_loop(s4_testloader, model1, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_loss2, test_acc4, test_acc5, test_acc6 = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i + 1}-------------------------------\")\n",
    "    model2.train()\n",
    "    train_loss2.append(train_loop(s_trainloader, model2, criterion, optimizer2))\n",
    "    model2.eval()\n",
    "    test_acc4.append(test_loop(o_testloader, model2, criterion))\n",
    "    test_acc5.append(test_loop(s_testloader, model2, criterion))\n",
    "    test_acc6.append(test_loop(s4_testloader, model2, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_loss3, test_acc7, test_acc8, test_acc9 = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i + 1}-------------------------------\")\n",
    "    model3.train()\n",
    "    train_loss3.append(train_loop(s4_trainloader, model3, criterion, optimizer3))\n",
    "    model3.eval()\n",
    "    test_acc7.append(test_loop(o_testloader, model3, criterion))\n",
    "    test_acc8.append(test_loop(s_testloader, model3, criterion))\n",
    "    test_acc9.append(test_loop(s4_testloader, model3, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result for Testing with original datasets\")\n",
    "print(max(test_acc1), max(test_acc2), max(test_acc3))\n",
    "\n",
    "print(\"Result for Testing with original datasets\")\n",
    "print(max(test_acc4), max(test_acc5), max(test_acc6))\n",
    "\n",
    "print(\"Result for Testing with original datasets\")\n",
    "print(max(test_acc7), max(test_acc8), max(test_acc9))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6bc9c0d9ed678a9439887435d1dc6423dbc20cda066c045b53c2a9575a7c6ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
